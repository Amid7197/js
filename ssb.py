# -*- coding: utf-8 -*-
import os
import sys
import requests
import time
import logging
import urllib3
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# å±è”½ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—ï¼šå°†çº§åˆ«è®¾ä¸º DEBUG ä»¥çœ‹åˆ°æ›´å¤šç»†èŠ‚
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG) 
formatter = logging.Formatter("%(asctime)s - [%(levelname)s] - %(message)s")
ch = logging.StreamHandler(sys.stdout)
ch.setFormatter(formatter)
logger.addHandler(ch)

def get_domain_from_userlist(file_path, line_number):
    try:
        if not os.path.exists(file_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
            return lines[line_number - 1].strip() if len(lines) >= line_number else None
    except Exception as e:
        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None

def get_refresh_url(current_url: str):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        }
        logger.info(f"æ­£åœ¨è¯·æ±‚: {current_url}")
        resp = requests.get(current_url, verify=False, timeout=15, headers=headers)
        resp.encoding = 'utf-8'
        html_content = resp.text

        # æ–¹æ¡ˆ A: æå…¶å¼ºæ‚çš„æ­£åˆ™åŒ¹é…
        # è§£é‡Šï¼šåŒ¹é… content å±æ€§ï¼Œå¿½ç•¥å‰é¢çš„ç§’æ•°(å¦‚0.1)ï¼Œç›´æ¥æŠ“å– url= ä¹‹åçš„å†…å®¹
        # èƒ½å¤Ÿå¤„ç†ï¼šcontent="0.1;url=/sou/go.html" æˆ– content='url=...' ç­‰å„ç§æƒ…å†µ
        refresh_pattern = re.compile(r'content=["\']?[\d.]*;\s*url=(.*?)["\']?[\s>]', re.IGNORECASE)
        match = refresh_pattern.search(html_content)
        
        if match:
            raw_url = match.group(1).strip().strip('"').strip("'").strip(';')
            full_url = urljoin(current_url, raw_url)
            logger.info(f"âœ¨ æ­£åˆ™æå–æˆåŠŸ: {full_url}")
            return full_url

        # æ–¹æ¡ˆ B: å…œåº•é€»è¾‘ - å¦‚æœæ­£åˆ™æ²¡æŠ“åˆ°ï¼Œå°è¯•æœç´¢ç®€å•çš„ url= å­—ç¬¦ä¸²
        if 'url=' in html_content.lower():
            try:
                # æš´åŠ›åˆ‡åˆ†å­—ç¬¦ä¸²æå–
                raw_url = html_content.lower().split('url=')[1].split('"')[0].split("'")[0].split('>')[0].strip()
                full_url = urljoin(current_url, raw_url)
                logger.info(f"ğŸ“ æš´åŠ›åˆ‡åˆ†æˆåŠŸ: {full_url}")
                return full_url
            except:
                pass

        logger.warning(f"âŒ è¿˜æ˜¯æ²¡æ‰¾åˆ°è·³è½¬ã€‚æºç ç‰‡æ®µ: {html_content[:100]}")
        return None
    except Exception as e:
        logger.error(f"æå–è¿‡ç¨‹å´©æºƒ: {e}")
        return None

# ... (check_connection, get_final_link_from_page, update_userlist ä¿æŒä¸å˜) ...

if __name__ == '__main__':
    domain = get_domain_from_userlist('userlist.txt', 3)
    if not domain:
        logger.critical("æ— æ³•ä» userlist.txt è·å–åŸŸåï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ï¼")
        sys.exit(1)

    # ç»Ÿä¸€ä½¿ç”¨ https
    current_step_url = domain if domain.startswith('http') else 'https://' + domain
    
    # è¿ç»­å°è¯•ä¸‰æ¬¡è·³è½¬
    for i in range(1, 4):
        logger.info(f"--- å°è¯•ç¬¬ {i} æ¬¡è·³è½¬è§£æ ---")
        next_url = get_refresh_url(current_step_url)
        if next_url:
            current_step_url = next_url
            time.sleep(2) # å¢åŠ å»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        else:
            break
    
    # æœ€ç»ˆç»“æœ
    logger.info(f"è·³è½¬æµç¨‹ç»“æŸï¼Œæœ€ç»ˆåœç•™åœ¨: {current_step_url}")
    # è¿™é‡Œè°ƒç”¨å¯»æ‰¾æŒ‰é’®çš„å‡½æ•°...
